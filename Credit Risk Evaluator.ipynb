{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Import Dependancies ###\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read in data ###\n",
    "train_df = pd.read_csv(Path('Resources/2019loans.csv'))\n",
    "test_df = pd.read_csv(Path('Resources/2020Q1loans.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert categorical data to numeric and separate target feature for training data\n",
    "# Convert categorical data to numeric and separate target feature for testing data\n",
    "train_df_X = train_df.drop(columns=['Unnamed: 0', 'index','loan_amnt'])\n",
    "test_df_X = test_df.drop(columns=['Unnamed: 0', 'index','loan_amnt'])\n",
    "train_df_y = train_df['loan_amnt'] \n",
    "test_df_y = test_df['loan_amnt']\n",
    "\n",
    "train_dummies = pd.get_dummies(train_df_X)\n",
    "test_dummies = pd.get_dummies(test_df_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add missing dummy variables to testing set\n",
    "train_col_list = train_dummies.columns.to_list()\n",
    "test_col_list = test_dummies.columns.to_list()\n",
    "train_col_df = pd.DataFrame({\"train\":train_col_list})\n",
    "test_col_df = pd.DataFrame({\"test\":test_col_list})\n",
    "checkDF = pd.concat([train_col_df,test_col_df],axis=1)\n",
    "\n",
    "def checkColumns(x):\n",
    "    train = x['train']\n",
    "    test = x['test']\n",
    "    if test == train:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "checkDF['col to test'] = checkDF.apply(lambda x: checkColumns(x),axis=1)\n",
    "\n",
    "coltotest = checkDF.loc[checkDF['col to test']==1]\n",
    "coltotest_list = coltotest['train'].tolist()\n",
    "\n",
    "for col in coltotest_list:\n",
    "    test_dummies[col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_dummies  \n",
    "X_test = test_dummies\n",
    "y_train = train_df_y\n",
    "y_test = test_df_y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Prediction\n",
    "---\n",
    "I predict that the random forest regression will have a better score because it chooses the optimal split point for each decision tree. It should provide higher accuracy through cross validation. Furthermore, logistic regression performs better with scaled data, which these models in this part are using unscaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.09688013136288999\n",
      "Testing Data Score: 0.06890684814972352\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train the Logistic Regression model on the unscaled data and print the model score\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train, y_train)\n",
    "print(f\"Training Data Score: {classifier.score(X_train, y_train)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test, y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n",
      "Testing Score: 0.3198638877073586\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model and print the model score\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_train, y_train)\n",
    "print(f'Training Score: {clf.score(X_train, y_train)}')\n",
    "print(f'Testing Score: {clf.score(X_test, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Result\n",
    "---\n",
    "The result is that logistic regression performed worse than random forest classifier. This was an expected finding because random forest can handle large data that isn't scaled. In addition, it has an approach that optimizes the decision trees it chooses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Prediction\n",
    "---\n",
    "I predict that the logistic regression will perform better with scaled data since it is a distance-based algorithm. However, it will still perform worse than the random forest classifier. Also the random forest classifer will perform about the same with the scaled data since it is a decision tree algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 0.4408866995073892\n",
      "Testing Score: 0.1837515950659294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train the Logistic Regression model on the scaled data and print the model score\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_train_scaled, y_train)\n",
    "print(f'Training Score: {classifier.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {classifier.score(X_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n",
      "Testing Score: 0.3200765631646108\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model on the scaled data and print the model score\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_train_scaled, y_train)\n",
    "print(f'Training Score: {clf.score(X_train_scaled, y_train)}')\n",
    "print(f'Testing Score: {clf.score(X_test_scaled, y_test)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Result\n",
    "---\n",
    "As expected, the results show that the logistic regression did perform significantly better with scaled data than when it was using unscaled data, which logically makes sense since it is a distanced-based algorithm. Thus, scaling the data will have a larger impact on the model score for a logistic regression model than a random forest classifier model. The other result is that the random forest classifer model using scaled data resulted in a very minor increase in the model score, which was expected since it uses a decision tree algorithm that chooses the optimal decision tree when building the model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
